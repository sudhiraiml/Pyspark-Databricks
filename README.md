# ðŸš€ PySpark-Databricks Learning Journey â€“ Beginner to Advanced

## âœ… Day 1: Big Data & Spark Intro
- What is Big Data?
- Spark Architecture
- Why PySpark?

## âœ… Day 2: Setup + Hello World
- Setup Databricks
- Create Cluster & Notebook
- `spark.range()`, `.printSchema()`

## âœ… Day 3: RDD vs DataFrame + CSV I/O
- RDDs vs DataFrames
- Read CSV â†’ `.read.option("header", True)`
- Write CSV â†’ `.write.mode("overwrite")`

## âœ… Day 4: Aggregations & groupBy
- `groupBy().agg()`
- `count`, `avg`, `max`, `min`
- Order results using `.orderBy()`

## âœ… Day 5: Joins, UDFs & withColumn
- `df1.join(df2, on="key", how="left")`
- `withColumn("new", col("existing") * 0.1)`
- UDF creation + registration

## âœ… Day 6: Window Functions + SQL
- `rank()`, `dense_rank()`, `row_number()` with `Window`
- `.createOrReplaceTempView()` + `spark.sql()`

## âœ… Day 7: File Formats & Partitioning
- Read/Write CSV, JSON, Parquet
- `partitionBy("col").write.parquet()`
- Best practices (Parquet preferred)

## âœ… Day 8: Performance Tuning
- `repartition()`, `coalesce()`
- `cache()`, `broadcast()`
- `.explain()` to analyze plan

## âœ… Day 9â€“10: Final Project
- Read employee + dept CSVs
- Join, clean, transform
- Window function + Aggregation
- Save final data as Parquet (partitioned)

> ðŸ’¡ Full project EDA â†’ Join â†’ Clean â†’ Transform â†’ Save  
> All code written in Databricks, production-style
